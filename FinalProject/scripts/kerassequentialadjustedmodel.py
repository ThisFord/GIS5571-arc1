# -*- coding: utf-8 -*-
"""KerasSequentialAdjustedModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C3vbaW6ZfprdpS8hXFDwvdNdudT10PeD
"""

# Inspiration and credit here:
# https://www.kaggle.com/ryanholbrook/starter-eurosat
# https://morioh.com/p/d1c01ca58d85

# Tensorflow datasets : https://www.tensorflow.org/datasets/overview
import tensorflow_datasets as tfds
import tensorflow as tf

# Split guide: https://www.tensorflow.org/datasets/splits
# If you want to split the dataset

# Download dataset.
ds_train, ds_info = tfds.load('eurosat/rgb', split='train', shuffle_files=True, with_info=True)

# Display a dataset sample

fig = tfds.show_examples(ds_train, ds_info)

# See how many examples we have for training and validation
print(ds_info.splits)

print(ds_info)

# Prepare our 'pipeline'

# Note: We can change this parameter.
BATCH_SIZE = 16


AUTO = tf.data.experimental.AUTOTUNE
SHUFFLE_BUFFER = int(ds_info.splits['train'].num_examples * 0.7)

# Split our data into training (70%) and validation (30%)
ds_train, ds_valid = tfds.load('eurosat/rgb',
                               split=['train[:70%]', 'train[70%:]'],
                               as_supervised=True)

# Mechanics
def preprocess(image, label):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

ds_train = (ds_train
            .map(preprocess, AUTO)
            .cache()
            .shuffle(SHUFFLE_BUFFER)
            .repeat()
            # Augmentations go here .map(augment, AUTO)
            .batch(BATCH_SIZE, drop_remainder=True)
            .prefetch(AUTO))

ds_valid = (ds_valid
            .map(preprocess, AUTO)
            .cache()
            .batch(BATCH_SIZE)
            .prefetch(AUTO))

import tensorflow.keras as keras
import tensorflow.keras.layers as layers

# How many label classes do we have
# Note: This is derived by the labels in our dataset. We do not set this unless we create the data.
NUM_CLASSES = ds_info.features['label'].num_classes
initializer = tf.keras.initializers.LecunNormal() #added initializer to deal with selu activation function


# build the DNN model
model = keras.Sequential([
    layers.BatchNormalization(),
    layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='elu'),
    layers.MaxPool2D(),

    layers.BatchNormalization(),
    layers.Conv2D(32, 3, padding='same', activation='elu'),
    layers.Conv2D(32, 3, padding='same', activation='elu'),
    layers.MaxPool2D(),

    layers.BatchNormalization(),
    layers.Conv2D(32, 3, padding='same', activation='elu'),
    layers.Conv2D(32, 3, padding='same', activation='elu'),
    layers.MaxPool2D(),
    
    layers.BatchNormalization(),
    layers.Conv2D(64, 3, padding='same', activation='elu'),
    layers.Conv2D(64, 3, padding='same', activation='elu'),
    layers.MaxPool2D(),
    
    layers.Flatten(),
    layers.Dense(128, activation='selu', kernel_initializer=initializer), # new activation function needs new initializer
    layers.AlphaDropout(0.5),
    layers.Dense(64, activation='selu', kernel_initializer=initializer),
    layers.AlphaDropout(0.5),
    layers.Dense(NUM_CLASSES, activation='softmax')
])

# change the model.compile lines to use different optimizer with other parameters
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy'],
)

# How much training?
EPOCHS = 4
STEPS_PER_EPOCH = int(ds_info.splits['train'].num_examples * 0.7) // BATCH_SIZE

######

print("Epochs:", EPOCHS)
print("Steps per Epoch:", STEPS_PER_EPOCH)

# Take a look at early stopping, it can help performance/training time
early_stopping = tf.keras.callbacks.EarlyStopping(patience=7, min_delta=0.001, restore_best_weights=True)

# Let's do this.
history = model.fit(
    ds_train,
    validation_data=ds_valid,
    epochs=EPOCHS,
    steps_per_epoch=STEPS_PER_EPOCH,
    callbacks=[early_stopping],
)

# Tell us about our model.
model.summary()

import pandas as pd

# Let's see how your model performed during training.

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['sparse_categorical_accuracy', 'val_sparse_categorical_accuracy']].plot()

# Understanding loss: https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss
# Loss tells us how we are doing on the training dataset
# Validation loss (val_loss) tells us how we are doing on the test/validation dataset.
# So focus on val_loss.

# Same goes for accuracy (acc)
# Focus on val_sparse_categorical_accuracy

"""### Optional

Below is additional code that shows how to pull the equivalent satellite imagery data from source. It does not have the 'mechanics' code to get it to run in the model, but is available for those who want to start deviating from the provided examples and datasets. 
"""

# Another example for inspiration

# Credit: https://medium.datadriveninvestor.com/patch-based-cover-type-classification-using-satellite-imagery-a67edeae7e24

import tensorflow as tf

from tensorflow.keras.models import Sequential 
from tensorflow.keras.backend import set_image_data_format 
from tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization 
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense 
from tensorflow.keras import optimizers, losses, utils 
#from livelossplot import keras_plot

!pip install wget
!pip install livelossplot

'''
# http://madm.dfki.de/files/sentinel/EuroSATallBands.zip

import wget
url = 'http://madm.dfki.de/files/sentinel/EuroSATallBands.zip'
filename = wget.download(url)

print(filename)
'''

# !ls eurodata/ds/images/remote_sensing/otherDatasets/sentinel_2/tif/AnnualCrop

#import shutil
#shutil.unpack_archive(filename,'eurodata')

set_image_data_format('channels_first') 
model = Sequential() 
model.add(Conv2D(28, (3, 3), padding='same',input_shape=(13, 64, 64))) 
model.add(Activation('relu')) 
model.add(Conv2D(28, (3, 3), padding='same')) 
model.add(Activation('relu')) 
model.add(MaxPool2D(2,2)) 
model.add(Conv2D(56, (3, 3),padding='same')) 
model.add(Activation('relu')) 
model.add(Conv2D(56, (3, 3), padding='same')) 
model.add(Activation('relu')) 
model.add(MaxPool2D(2,2)) 
model.add(Conv2D(112, (3, 3), padding='same')) 
model.add(Activation('relu')) 
model.add(Conv2D(112, (3, 3), padding='same')) 
model.add(Activation('relu')) 
model.add(MaxPool2D(2,2)) 
model.add(Flatten()) 
model.add(Dense(784)) 
model.add(Activation('relu')) 
model.add(Dropout(0.6)) 
model.add(Dense(10)) 
model.add(Activation('sigmoid')) 
adam = optimizers.Adam(learning_rate=0.001) 
model.compile(optimizer=adam, loss=losses.binary_crossentropy, metrics=['accuracy'])

model.summary()